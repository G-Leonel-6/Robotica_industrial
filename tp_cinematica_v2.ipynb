{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bKlbZRtlkFE3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import scipy as sc\n",
        "import roboticstoolbox as rtb\n",
        "import spatialmath as sm\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.optimize import fsolve\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "Gcii7kl2kFGo"
      },
      "outputs": [],
      "source": [
        "class irb140_clase(rtb.DHRobot):\n",
        "    def __init__(self):\n",
        "        super().__init__([\n",
        "            rtb.RevoluteDH(alpha=-np.pi/2,a=0.07, qlim=[-np.pi,np.pi]),\n",
        "            rtb.RevoluteDH(a=0.36,offset=-np.pi/2, qlim=[-np.pi,np.pi]),\n",
        "            rtb.RevoluteDH(alpha=np.pi/2,offset=np.pi, qlim=[-np.pi,np.pi]),\n",
        "            rtb.RevoluteDH(d=0.38, alpha=-np.pi/2, qlim=[-np.pi,np.pi]),\n",
        "            rtb.RevoluteDH(alpha=np.pi/2, qlim=[-np.pi,np.pi]),\n",
        "            rtb.RevoluteDH()\n",
        "        ], name=\"IRB140\")\n",
        "\n",
        "    def get_config(self,q):\n",
        "        g1 = np.sign(self.links[3].d * np.sin(q[1]+self.links[1].offset +q[2]+self.links[2].offset) + self.links[1].a * np.cos(q[1]+self.links[1].offset) + self.links[0].a)\n",
        "        g2 = np.sign(np.cos(q[2]+self.links[2].offset))\n",
        "        g3 = np.sign(np.sin(q[4]+self.links[4].offset))\n",
        "        return np.array([g1,g2,g3])\n",
        "\n",
        "    def get_joint_bounds(self):\n",
        "        return np.array([link.qlim if link.qlim is not None else [-np.pi, np.pi] for link in self.links])\n",
        "\n",
        "    def solve_q1(self, g1, q2):\n",
        "        d3, o1, o2, a1, a0 = self.links[3].d, self.links[1].offset, self.links[2].offset, self.links[1].a, self.links[0].a\n",
        "        def S(theta): return d3 * np.sin(theta + q2 + o2) + a1 * np.cos(theta) + a0\n",
        "        sol = fsolve(S, 0.0)[0]\n",
        "        theta = sol + np.pi/6 if g1 * S(sol + np.pi/6) > 0 else sol - np.pi/6\n",
        "        return theta - o1\n",
        "\n",
        "    def guess_seed_from_conf(self, conf,POSE ,q_ref=None):\n",
        "        q0 = np.zeros(6)\n",
        "\n",
        "        px, py, pz = POSE.t\n",
        "        g1 = conf[0]\n",
        "        if px >= 0 and py >= 0: \n",
        "            q0[0] = np.pi/4 if g1 == 1 else -3*np.pi/4\n",
        "        elif px >= 0 and py < 0:  \n",
        "            q0[0] = -np.pi/4 if g1 == 1 else 3*np.pi/4\n",
        "        elif px < 0 and py >= 0:  \n",
        "            q0[0] = 3*np.pi/4 if g1 == 1 else -np.pi/4\n",
        "        else:  \n",
        "            q0[0] = -3*np.pi/4 if g1 == 1 else np.pi/4\n",
        "\n",
        "        q0[2] = np.pi/2 + conf[1] * np.pi/3\n",
        "        q0[4] = conf[2] * np.pi/4\n",
        "        q0[1] = self.solve_q1(conf[0], q0[2])\n",
        "        return 0.5*q0 + 0.5*q_ref if q_ref is not None else q0\n",
        "\n",
        "    def wrap_joints(self, q):\n",
        "        q_wrapped = np.copy(q)\n",
        "        bounds = self.get_joint_bounds()\n",
        "        for i in range(len(q)):\n",
        "            q_min, q_max = bounds[i]\n",
        "            span = q_max - q_min\n",
        "            q_wrapped[i] = (q[i] - q_min) % span + q_min\n",
        "        return q_wrapped\n",
        "\n",
        "    def damped_pinv(self, J, damping=1e-4):\n",
        "        U, S, Vh = np.linalg.svd(J)\n",
        "        S_inv = S / (S**2 + damping**2)\n",
        "        return Vh.T @ np.diag(S_inv) @ U.T\n",
        "\n",
        "    def config_match(self, q, conf):\n",
        "        return np.all(self.get_config(q) == conf)\n",
        "\n",
        "    def ikine_recta_con_trayectoria(self, POSE_deseada, conf, q_inicial=None,\n",
        "                                    vd=0.01, Ts=0.001, tol=1e-3, max_iter=2000,\n",
        "                                    q_actual=None):\n",
        "        q_i = q_inicial if q_inicial is not None else (\n",
        "            q_actual.copy() if q_actual is not None else self.guess_seed_from_conf(conf, POSE_deseada)\n",
        "        )\n",
        "        trayectoria = [q_i]\n",
        "        errores = []\n",
        "\n",
        "        for i in range(max_iter):\n",
        "            POSE_i = self.fkine(q_i)\n",
        "            e_vec = sm.base.tr2delta(POSE_i.A, POSE_deseada.A)\n",
        "            error_norm = np.linalg.norm(e_vec)\n",
        "            errores.append(error_norm)\n",
        "\n",
        "            if error_norm < tol:\n",
        "                return (q_i, 0, np.array(trayectoria), errores) if self.config_match(q_i, conf) else (q_i, -2, np.array(trayectoria), errores)\n",
        "\n",
        "            delta_pose_i = (e_vec / error_norm) * vd * Ts\n",
        "            J = self.jacobe(q_i)\n",
        "            dq_i = self.damped_pinv(J) @ delta_pose_i\n",
        "            q_i = self.wrap_joints(q_i + dq_i)\n",
        "            trayectoria.append(q_i)\n",
        "\n",
        "        return q_i, -1, np.array(trayectoria), errores\n",
        "\n",
        "    def ikine_recta_con_trayectoria_multi(self, POSE_deseada, conf, q_actual=None,\n",
        "                                          vd=0.01, Ts=0.001, tol=1e-3, max_iter=2000,\n",
        "                                          n_reintentos=6):\n",
        "        for intento in range(n_reintentos):\n",
        "            if intento == 0 and q_actual is not None:\n",
        "                q_seed = q_actual\n",
        "            elif intento == 1:\n",
        "                q_seed = self.guess_seed_from_conf(conf, POSE_deseada)\n",
        "            else:\n",
        "                perturb = (np.random.rand(6)-0.5)*0.2\n",
        "                q_seed = self.guess_seed_from_conf(conf, POSE_deseada) + perturb\n",
        "\n",
        "            q, status, trayectoria, errores = self.ikine_recta_con_trayectoria(\n",
        "                POSE_deseada, conf, q_inicial=q_seed, vd=vd, Ts=Ts, tol=tol, max_iter=max_iter\n",
        "            )\n",
        "            if status == 0:\n",
        "                return q, status, trayectoria, errores\n",
        "        return q, status, trayectoria, errores\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "xd0XgpVXkFJL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Cantidad de corridas: 1\n",
            "🎯 Cantidad de aciertos: 1\n"
          ]
        }
      ],
      "source": [
        "robot = irb140_clase()\n",
        "n_acierto = 0\n",
        "n_iter = 1\n",
        "\n",
        "for i in range(n_iter):\n",
        "    q_deseado = (np.random.rand(6)-0.5)*2*np.pi\n",
        "    POSE = robot.fkine(q_deseado)\n",
        "    conf = robot.get_config(q_deseado)\n",
        "    q, success, history, _ = robot.ikine_recta_con_trayectoria_multi(\n",
        "        POSE, conf=conf, vd=1, Ts=0.001, max_iter=8000, n_reintentos=10\n",
        "    )\n",
        "    if success == 0:\n",
        "        n_acierto += 1\n",
        "    else:\n",
        "        print(f\"❌ ERROR:\\nDeseado: {q_deseado}\\nConfig: {conf}\\nAlcanzado: {q}\")\n",
        "\n",
        "print(\"✅ Cantidad de corridas:\", n_iter)\n",
        "print(\"🎯 Cantidad de aciertos:\", n_acierto)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/300 - Loss: 3.997307\n",
            "Epoch 2/300 - Loss: 2.668416\n",
            "Epoch 3/300 - Loss: 2.458839\n",
            "Epoch 4/300 - Loss: 2.211545\n",
            "Epoch 5/300 - Loss: 1.947850\n",
            "Epoch 6/300 - Loss: 1.799505\n",
            "Epoch 7/300 - Loss: 1.661745\n",
            "Epoch 8/300 - Loss: 1.555153\n",
            "Epoch 9/300 - Loss: 1.472501\n",
            "Epoch 10/300 - Loss: 1.407245\n",
            "Epoch 11/300 - Loss: 1.328690\n",
            "Epoch 12/300 - Loss: 1.248085\n",
            "Epoch 13/300 - Loss: 1.178978\n",
            "Epoch 14/300 - Loss: 1.134945\n",
            "Epoch 15/300 - Loss: 1.099222\n",
            "Epoch 16/300 - Loss: 1.071633\n",
            "Epoch 17/300 - Loss: 1.041620\n",
            "Epoch 18/300 - Loss: 1.018390\n",
            "Epoch 19/300 - Loss: 0.994287\n",
            "Epoch 20/300 - Loss: 0.974190\n",
            "Epoch 21/300 - Loss: 0.952761\n",
            "Epoch 22/300 - Loss: 0.934326\n",
            "Epoch 23/300 - Loss: 0.914403\n",
            "Epoch 24/300 - Loss: 0.902552\n",
            "Epoch 25/300 - Loss: 0.888559\n",
            "Epoch 26/300 - Loss: 0.875113\n",
            "Epoch 27/300 - Loss: 0.863425\n",
            "Epoch 28/300 - Loss: 0.857809\n",
            "Epoch 29/300 - Loss: 0.843561\n",
            "Epoch 30/300 - Loss: 0.837095\n",
            "Epoch 31/300 - Loss: 0.827196\n",
            "Epoch 32/300 - Loss: 0.818379\n",
            "Epoch 33/300 - Loss: 0.810406\n",
            "Epoch 34/300 - Loss: 0.802736\n",
            "Epoch 35/300 - Loss: 0.798846\n",
            "Epoch 36/300 - Loss: 0.787684\n",
            "Epoch 37/300 - Loss: 0.780769\n",
            "Epoch 38/300 - Loss: 0.773647\n",
            "Epoch 39/300 - Loss: 0.766711\n",
            "Epoch 40/300 - Loss: 0.758935\n",
            "Epoch 41/300 - Loss: 0.753686\n",
            "Epoch 42/300 - Loss: 0.745626\n",
            "Epoch 43/300 - Loss: 0.738789\n",
            "Epoch 44/300 - Loss: 0.733248\n",
            "Epoch 45/300 - Loss: 0.728377\n",
            "Epoch 46/300 - Loss: 0.721883\n",
            "Epoch 47/300 - Loss: 0.715810\n",
            "Epoch 48/300 - Loss: 0.707003\n",
            "Epoch 49/300 - Loss: 0.703197\n",
            "Epoch 50/300 - Loss: 0.699017\n",
            "Epoch 51/300 - Loss: 0.694251\n",
            "Epoch 52/300 - Loss: 0.688727\n",
            "Epoch 53/300 - Loss: 0.686169\n",
            "Epoch 54/300 - Loss: 0.680479\n",
            "Epoch 55/300 - Loss: 0.677178\n",
            "Epoch 56/300 - Loss: 0.671378\n",
            "Epoch 57/300 - Loss: 0.662480\n",
            "Epoch 58/300 - Loss: 0.659138\n",
            "Epoch 59/300 - Loss: 0.656426\n",
            "Epoch 60/300 - Loss: 0.652111\n",
            "Epoch 61/300 - Loss: 0.648266\n",
            "Epoch 62/300 - Loss: 0.643512\n",
            "Epoch 63/300 - Loss: 0.642166\n",
            "Epoch 64/300 - Loss: 0.636640\n",
            "Epoch 65/300 - Loss: 0.632321\n",
            "Epoch 66/300 - Loss: 0.631368\n",
            "Epoch 67/300 - Loss: 0.627384\n",
            "Epoch 68/300 - Loss: 0.620779\n",
            "Epoch 69/300 - Loss: 0.619362\n",
            "Epoch 70/300 - Loss: 0.616474\n",
            "Epoch 71/300 - Loss: 0.611548\n",
            "Epoch 72/300 - Loss: 0.607855\n",
            "Epoch 73/300 - Loss: 0.605425\n",
            "Epoch 74/300 - Loss: 0.602251\n",
            "Epoch 75/300 - Loss: 0.597038\n",
            "Epoch 76/300 - Loss: 0.596442\n",
            "Epoch 77/300 - Loss: 0.589645\n",
            "Epoch 78/300 - Loss: 0.591204\n",
            "Epoch 79/300 - Loss: 0.586634\n",
            "Epoch 80/300 - Loss: 0.585851\n",
            "Epoch 81/300 - Loss: 0.581912\n",
            "Epoch 82/300 - Loss: 0.577080\n",
            "Epoch 83/300 - Loss: 0.577065\n",
            "Epoch 84/300 - Loss: 0.572400\n",
            "Epoch 85/300 - Loss: 0.568227\n",
            "Epoch 86/300 - Loss: 0.568175\n",
            "Epoch 87/300 - Loss: 0.564459\n",
            "Epoch 88/300 - Loss: 0.563772\n",
            "Epoch 89/300 - Loss: 0.560854\n",
            "Epoch 90/300 - Loss: 0.558151\n",
            "Epoch 91/300 - Loss: 0.555818\n",
            "Epoch 92/300 - Loss: 0.554612\n",
            "Epoch 93/300 - Loss: 0.549119\n",
            "Epoch 94/300 - Loss: 0.547521\n",
            "Epoch 95/300 - Loss: 0.542478\n",
            "Epoch 96/300 - Loss: 0.543290\n",
            "Epoch 97/300 - Loss: 0.538398\n",
            "Epoch 98/300 - Loss: 0.540823\n",
            "Epoch 99/300 - Loss: 0.536093\n",
            "Epoch 100/300 - Loss: 0.531349\n",
            "Epoch 101/300 - Loss: 0.535942\n",
            "Epoch 102/300 - Loss: 0.530208\n",
            "Epoch 103/300 - Loss: 0.527214\n",
            "Epoch 104/300 - Loss: 0.525493\n",
            "Epoch 105/300 - Loss: 0.522998\n",
            "Epoch 106/300 - Loss: 0.519799\n",
            "Epoch 107/300 - Loss: 0.518708\n",
            "Epoch 108/300 - Loss: 0.519625\n",
            "Epoch 109/300 - Loss: 0.515302\n",
            "Epoch 110/300 - Loss: 0.509476\n",
            "Epoch 111/300 - Loss: 0.511917\n",
            "Epoch 112/300 - Loss: 0.512205\n",
            "Epoch 113/300 - Loss: 0.506652\n",
            "Epoch 114/300 - Loss: 0.508247\n",
            "Epoch 115/300 - Loss: 0.507026\n",
            "Epoch 116/300 - Loss: 0.507149\n",
            "Epoch 117/300 - Loss: 0.503844\n",
            "Epoch 118/300 - Loss: 0.498680\n",
            "Epoch 119/300 - Loss: 0.493697\n",
            "Epoch 120/300 - Loss: 0.496264\n",
            "Epoch 121/300 - Loss: 0.500415\n",
            "Epoch 122/300 - Loss: 0.492574\n",
            "Epoch 123/300 - Loss: 0.495375\n",
            "Epoch 124/300 - Loss: 0.487827\n",
            "Epoch 125/300 - Loss: 0.489664\n",
            "Epoch 126/300 - Loss: 0.485332\n",
            "Epoch 127/300 - Loss: 0.483922\n",
            "Epoch 128/300 - Loss: 0.485434\n",
            "Epoch 129/300 - Loss: 0.485155\n",
            "Epoch 130/300 - Loss: 0.479898\n",
            "Epoch 131/300 - Loss: 0.483125\n",
            "Epoch 132/300 - Loss: 0.476568\n",
            "Epoch 133/300 - Loss: 0.477739\n",
            "Epoch 134/300 - Loss: 0.474704\n",
            "Epoch 135/300 - Loss: 0.473682\n",
            "Epoch 136/300 - Loss: 0.474416\n",
            "Epoch 137/300 - Loss: 0.469603\n",
            "Epoch 138/300 - Loss: 0.469448\n",
            "Epoch 139/300 - Loss: 0.466019\n",
            "Epoch 140/300 - Loss: 0.464803\n",
            "Epoch 141/300 - Loss: 0.467506\n",
            "Epoch 142/300 - Loss: 0.464689\n",
            "Epoch 143/300 - Loss: 0.461508\n",
            "Epoch 144/300 - Loss: 0.459760\n",
            "Epoch 145/300 - Loss: 0.458567\n",
            "Epoch 146/300 - Loss: 0.456454\n",
            "Epoch 147/300 - Loss: 0.458150\n",
            "Epoch 148/300 - Loss: 0.456540\n",
            "Epoch 149/300 - Loss: 0.453844\n",
            "Epoch 150/300 - Loss: 0.456362\n",
            "Epoch 151/300 - Loss: 0.455534\n",
            "Epoch 152/300 - Loss: 0.448961\n",
            "Epoch 153/300 - Loss: 0.450895\n",
            "Epoch 154/300 - Loss: 0.448511\n",
            "Epoch 155/300 - Loss: 0.448329\n",
            "Epoch 156/300 - Loss: 0.446389\n",
            "Epoch 157/300 - Loss: 0.448656\n",
            "Epoch 158/300 - Loss: 0.443877\n",
            "Epoch 159/300 - Loss: 0.442565\n",
            "Epoch 160/300 - Loss: 0.441245\n",
            "Epoch 161/300 - Loss: 0.438801\n",
            "Epoch 162/300 - Loss: 0.438873\n",
            "Epoch 163/300 - Loss: 0.436311\n",
            "Epoch 164/300 - Loss: 0.443183\n",
            "Epoch 165/300 - Loss: 0.439594\n",
            "Epoch 166/300 - Loss: 0.438344\n",
            "Epoch 167/300 - Loss: 0.432342\n",
            "Epoch 168/300 - Loss: 0.434835\n",
            "Epoch 169/300 - Loss: 0.431127\n",
            "Epoch 170/300 - Loss: 0.430380\n",
            "Epoch 171/300 - Loss: 0.429292\n",
            "Epoch 172/300 - Loss: 0.430185\n",
            "Epoch 173/300 - Loss: 0.430040\n",
            "Epoch 174/300 - Loss: 0.429356\n",
            "Epoch 175/300 - Loss: 0.424764\n",
            "Epoch 176/300 - Loss: 0.424323\n",
            "Epoch 177/300 - Loss: 0.423685\n",
            "Epoch 178/300 - Loss: 0.422733\n",
            "Epoch 179/300 - Loss: 0.421207\n",
            "Epoch 180/300 - Loss: 0.425944\n",
            "Epoch 181/300 - Loss: 0.418610\n",
            "Epoch 182/300 - Loss: 0.420823\n",
            "Epoch 183/300 - Loss: 0.422441\n",
            "Epoch 184/300 - Loss: 0.416941\n",
            "Epoch 185/300 - Loss: 0.416105\n",
            "Epoch 186/300 - Loss: 0.412494\n",
            "Epoch 187/300 - Loss: 0.413137\n",
            "Epoch 188/300 - Loss: 0.412865\n",
            "Epoch 189/300 - Loss: 0.420074\n",
            "Epoch 190/300 - Loss: 0.413622\n",
            "Epoch 191/300 - Loss: 0.410105\n",
            "Epoch 192/300 - Loss: 0.408662\n",
            "Epoch 193/300 - Loss: 0.409455\n",
            "Epoch 194/300 - Loss: 0.405571\n",
            "Epoch 195/300 - Loss: 0.409948\n",
            "Epoch 196/300 - Loss: 0.408007\n",
            "Epoch 197/300 - Loss: 0.408546\n",
            "Epoch 198/300 - Loss: 0.409164\n",
            "Epoch 199/300 - Loss: 0.405346\n",
            "Epoch 200/300 - Loss: 0.402421\n",
            "Epoch 201/300 - Loss: 0.400535\n",
            "Epoch 202/300 - Loss: 0.400886\n",
            "Epoch 203/300 - Loss: 0.400876\n",
            "Epoch 204/300 - Loss: 0.400134\n",
            "Epoch 205/300 - Loss: 0.400319\n",
            "Epoch 206/300 - Loss: 0.396517\n",
            "Epoch 207/300 - Loss: 0.395838\n",
            "Epoch 208/300 - Loss: 0.398130\n",
            "Epoch 209/300 - Loss: 0.395135\n",
            "Epoch 210/300 - Loss: 0.394373\n",
            "Epoch 211/300 - Loss: 0.394125\n",
            "Epoch 212/300 - Loss: 0.392015\n",
            "Epoch 213/300 - Loss: 0.396317\n",
            "Epoch 214/300 - Loss: 0.396233\n",
            "Epoch 215/300 - Loss: 0.394360\n",
            "Epoch 216/300 - Loss: 0.391010\n",
            "Epoch 217/300 - Loss: 0.388912\n",
            "Epoch 218/300 - Loss: 0.389236\n",
            "Epoch 219/300 - Loss: 0.388710\n",
            "Epoch 220/300 - Loss: 0.388349\n",
            "Epoch 221/300 - Loss: 0.385589\n",
            "Epoch 222/300 - Loss: 0.386429\n",
            "Epoch 223/300 - Loss: 0.385739\n",
            "Epoch 224/300 - Loss: 0.384345\n",
            "Epoch 225/300 - Loss: 0.387665\n",
            "Epoch 226/300 - Loss: 0.382984\n",
            "Epoch 227/300 - Loss: 0.390337\n",
            "Epoch 228/300 - Loss: 0.384940\n",
            "Epoch 229/300 - Loss: 0.380310\n",
            "Epoch 230/300 - Loss: 0.381079\n",
            "Epoch 231/300 - Loss: 0.376660\n",
            "Epoch 232/300 - Loss: 0.382585\n",
            "Epoch 233/300 - Loss: 0.382738\n",
            "Epoch 234/300 - Loss: 0.379030\n",
            "Epoch 235/300 - Loss: 0.376067\n",
            "Epoch 236/300 - Loss: 0.379051\n",
            "Epoch 237/300 - Loss: 0.376082\n",
            "Epoch 238/300 - Loss: 0.377430\n",
            "Epoch 239/300 - Loss: 0.376570\n",
            "Epoch 240/300 - Loss: 0.374826\n",
            "Epoch 241/300 - Loss: 0.371817\n",
            "Epoch 242/300 - Loss: 0.372890\n",
            "Epoch 243/300 - Loss: 0.372487\n",
            "Epoch 244/300 - Loss: 0.371203\n",
            "Epoch 245/300 - Loss: 0.374882\n",
            "Epoch 246/300 - Loss: 0.370052\n",
            "Epoch 247/300 - Loss: 0.369547\n",
            "Epoch 248/300 - Loss: 0.372421\n",
            "Epoch 249/300 - Loss: 0.367273\n",
            "Epoch 250/300 - Loss: 0.365962\n",
            "Epoch 251/300 - Loss: 0.369955\n",
            "Epoch 252/300 - Loss: 0.368164\n",
            "Epoch 253/300 - Loss: 0.369620\n",
            "Epoch 254/300 - Loss: 0.369322\n",
            "Epoch 255/300 - Loss: 0.365080\n",
            "Epoch 256/300 - Loss: 0.367227\n",
            "Epoch 257/300 - Loss: 0.364397\n",
            "Epoch 258/300 - Loss: 0.364048\n",
            "Epoch 259/300 - Loss: 0.368193\n",
            "Epoch 260/300 - Loss: 0.362031\n",
            "Epoch 261/300 - Loss: 0.362372\n",
            "Epoch 262/300 - Loss: 0.361644\n",
            "Epoch 263/300 - Loss: 0.361254\n",
            "Epoch 264/300 - Loss: 0.359970\n",
            "Epoch 265/300 - Loss: 0.363720\n",
            "Epoch 266/300 - Loss: 0.361165\n",
            "Epoch 267/300 - Loss: 0.361013\n",
            "Epoch 268/300 - Loss: 0.358667\n",
            "Epoch 269/300 - Loss: 0.356359\n",
            "Epoch 270/300 - Loss: 0.357388\n",
            "Epoch 271/300 - Loss: 0.356926\n",
            "Epoch 272/300 - Loss: 0.354704\n",
            "Epoch 273/300 - Loss: 0.359780\n",
            "Epoch 274/300 - Loss: 0.353304\n",
            "Epoch 275/300 - Loss: 0.354552\n",
            "Epoch 276/300 - Loss: 0.350587\n",
            "Epoch 277/300 - Loss: 0.355558\n",
            "Epoch 278/300 - Loss: 0.352465\n",
            "Epoch 279/300 - Loss: 0.349038\n",
            "Epoch 280/300 - Loss: 0.353308\n",
            "Epoch 281/300 - Loss: 0.352479\n",
            "Epoch 282/300 - Loss: 0.353002\n",
            "Epoch 283/300 - Loss: 0.350892\n",
            "Epoch 284/300 - Loss: 0.351665\n",
            "Epoch 285/300 - Loss: 0.349343\n",
            "Epoch 286/300 - Loss: 0.351787\n",
            "Epoch 287/300 - Loss: 0.349057\n",
            "Epoch 288/300 - Loss: 0.347961\n",
            "Epoch 289/300 - Loss: 0.349463\n",
            "Epoch 290/300 - Loss: 0.347650\n",
            "Epoch 291/300 - Loss: 0.347500\n",
            "Epoch 292/300 - Loss: 0.346295\n",
            "Epoch 293/300 - Loss: 0.344804\n",
            "Epoch 294/300 - Loss: 0.346017\n",
            "Epoch 295/300 - Loss: 0.344451\n",
            "Epoch 296/300 - Loss: 0.347316\n",
            "Epoch 297/300 - Loss: 0.344644\n",
            "Epoch 298/300 - Loss: 0.347064\n",
            "Epoch 299/300 - Loss: 0.345443\n",
            "Epoch 300/300 - Loss: 0.340054\n",
            "Error medio absoluto: 0.3508 rad\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import spatialmath as sm\n",
        "import roboticstoolbox as rtb\n",
        "\n",
        "# --- Paso 1: Generar datos ---\n",
        "def generar_dataset(robot, n_samples=100000):\n",
        "    qs = np.random.uniform(0, 2*np.pi, (n_samples, 6))\n",
        "    poses = []\n",
        "    confs = []\n",
        "\n",
        "    for q in qs:\n",
        "        T = robot.fkine(q)\n",
        "        pose = np.hstack((T.t, T.R.flatten()))\n",
        "        conf = robot.get_config(q)\n",
        "        poses.append(np.hstack((pose, conf)))\n",
        "\n",
        "    X = np.array(poses, dtype=np.float32)\n",
        "    y = np.array(qs, dtype=np.float32)\n",
        "    return X, y\n",
        "\n",
        "# --- Paso 2: Definir la red ---\n",
        "class IKNet(nn.Module):\n",
        "    def __init__(self, input_dim=15, joint_bounds=None, hidden=256):\n",
        "        super().__init__()\n",
        "        self.joint_bounds = torch.tensor(joint_bounds, dtype=torch.float32)\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 6)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.net(x)\n",
        "        return y\n",
        "\n",
        "# --- Paso 3: Entrenamiento ---\n",
        "def entrenar_red(robot, epochs=300, batch_size=1024):\n",
        "    X, y = generar_dataset(robot)\n",
        "    X_train = torch.tensor(X)\n",
        "    y_train = torch.tensor(y)\n",
        "\n",
        "    dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
        "    loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    model = IKNet(input_dim=X.shape[1], joint_bounds=robot.get_joint_bounds())\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "    loss_fn = nn.MSELoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for xb, yb in loader:\n",
        "            pred = model(xb)\n",
        "            loss = loss_fn(pred, yb)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}/{epochs} - Loss: {total_loss/len(loader):.6f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# --- Paso 4: Evaluación ---\n",
        "def evaluar(model, robot, n_test=100):\n",
        "    X_test, y_test = generar_dataset(robot, n_samples=n_test)\n",
        "    X_t = torch.tensor(X_test)\n",
        "    y_pred = model(X_t).detach().numpy()\n",
        "\n",
        "    errores = np.abs((y_pred - y_test + np.pi) % (2*np.pi) - np.pi)\n",
        "    mae = np.mean(errores)\n",
        "    print(f\"Error medio absoluto: {mae:.4f} rad\")\n",
        "    return mae\n",
        "\n",
        "# --- Uso ---\n",
        "if __name__ == \"__main__\":\n",
        "    robot = irb140_clase()\n",
        "    model = entrenar_red(robot, epochs=300)\n",
        "    evaluar(model, robot)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deseado: [ 2.68344555 -3.11645747  1.07200485  0.73256333 -1.031064    0.13795487]\n",
            "Predicho: [2.802371   3.1434662  1.0736734  0.57491785 5.1163034  1.8825862 ]\n"
          ]
        }
      ],
      "source": [
        "q_deseado = (np.random.rand(6)-0.5)*2*np.pi\n",
        "POSE = robot.fkine(q_deseado)\n",
        "conf = robot.get_config(q_deseado)\n",
        "pose = np.hstack((POSE.t, POSE.R.flatten()))\n",
        "x = np.hstack((pose, conf))\n",
        "y = model(torch.tensor(x, dtype=torch.float32)).detach().numpy()\n",
        "print(\"Deseado:\", q_deseado)\n",
        "print(\"Predicho:\", y)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
